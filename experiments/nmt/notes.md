- 15000 most frequent words for both English and French
- 1000 hidden units
- 100 dimension word embedding
- 500 maxout units each pooling 2 inputs from hidden state in decoder to output
- weights initialized with zero-mean Gaussian except for the recurrent weights, which are sample by left singular vector matrix of zero-mean Gaussian (Saxe el.al., 2014)
- c_dim = number of components * hidden layer size; forward and backward trainings don't interact
- repr_contributors and maxout layers are very strange
- forward and backward RNNs in parallel for encoder, but not used in experiments
- tricks
	- bias
	- weight noise
	- gradient clipping: cutoff and cutoff_rescale_length
	- sort sentences on length in minibatch
