- 15000 most frequent words for both English and French
- 1000 hidden units
- 100 dimension word embedding
- 500 maxout units each pooling 2 inputs from hidden state in decoder to output
- weights initialized with zero-mean Gaussian except for the recurrent weights, which are sample by left singular vector matrix of zero-mean Gaussian (Saxe el.al., 2014)
- forward and backward RNNs in parallel for encoder, only last hidden layer of forward training is used in experiments
- c_dim = number of components * hidden layer size; forward and backward trainings don't interact
- batch size 64
- when used for scoring phrase pairs, score = -cost

- bias not learned:
	- encoder
		- update and reset embedder
	- decoder
		- decoding embedders
		- from decoded context to softmax
		- from previous word embedding to softmax
- weight noise: not used
- gradient clipping: cutoff and cutoff_rescale_length
- sort sentences by length in minibatch
