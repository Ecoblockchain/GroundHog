- 15000 most frequent words for both English and French
- 1000 hidden units
- 100 dimension word embedding
- 500 maxout units each pooling 2 inputs from hidden state in decoder to output
- weights initialized with zero-mean Gaussian except for the recurrent weights, which are sample by left singular vector matrix of zero-mean Gaussian (Saxe el.al., 2014)
- forward and backward RNNs in parallel for encoder, only last hidden layer of forward training is used in experiments
- c_dim = number of components * hidden layer size; forward and backward trainings don't interact
- batch size 64
- when used for scoring phrase pairs, score = -cost

- bias not learned:
	- encoder
		- update and reset embedder
	- decoder
		- decoding embedders
		- from decoded context to softmax
		- from previous word embedding to softmax
- weight noise: not used
- gradient clipping: cutoff and cutoff_rescale_length
- sort sentences by length in minibatch
- there should be only one weight matrix from word embedding to each gate, but there are two
- rank n approx between maxout and softmax

- gradient calculated at cost level automatically by theano tensor: T.grad(cost.mean(), self.params())
- for character level sampling, change LM_Model
- sampling: lm_model <- RNNEncoderDecoder.create_sampler <- Decoder.build_sampler
- gradient: theano.tensor.grads <- cost_layer.get_grads <- basic_layer.train <- RNNEncoderDecoder.buildDecoder <- RNNEncoderDecoder.build (highest level)
- when there is only one GRU (not stacked), the Maxout between encoder hidden and context is not used
